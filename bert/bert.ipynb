{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 768  # the hidden embedding dimension, the Embedding size\n",
    "num_segments = 2  # number of segments indicates which sentence it belongs to\n",
    "max_length = 30  # maximum length of a sentence\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "n_heads = 8  # number of heads in Multi-Head Attention\n",
    "batch_size = 6\n",
    "max_predict = 5  # max tokens of prediction\n",
    "n_layers = 6  # number of Encoder of Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n'\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "    'Nice meet you too. How are you today?\\n'\n",
    "    'Great. My baseball team won the competition.\\n'\n",
    "    'Oh Congratulations, Juliet\\n'\n",
    "    'Thanks you Romeo'\n",
    ")\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample `IsNext` and `NotNext` to be same in small batch size\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive, negative = 0, 0\n",
    "    \n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "        # sample random index in sentences\n",
    "        tokens_a_idx, tokens_b_idx = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_idx], token_list[tokens_b_idx]\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        ''' Mask LM '''\n",
    "        n_predict = min(max_predict, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        cand_maked_pos = [\n",
    "            i for i, token in enumerate(input_ids)\n",
    "            if token != word_dict['[CLS]'] and token != [word_dict['[SEP]']]\n",
    "        ]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_predict]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:\n",
    "                input_ids[pos] = [word_dict['[MASK]']]\n",
    "            elif random() < 0.5:\n",
    "                idx = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = word_dict[number_dict[idx]]\n",
    "        \n",
    "\n",
    "        ''' Zero Padding '''\n",
    "        num_pad = max_length - len(input_ids)\n",
    "        input_ids.extend([0] * num_pad)\n",
    "        segment_ids.extend([0] * num_pad)\n",
    "\n",
    "        # Padding the (1 - 0.15) tokens \n",
    "        if max_length > num_pad:\n",
    "            num_pad = max_length - n_predict\n",
    "            masked_tokens.extend([0] * num_pad)\n",
    "            masked_pos.extend([0] * num_pad)\n",
    "\n",
    "        if tokens_a_idx + 1 == tokens_b_idx and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])  # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_idx + 1 != tokens_b_idx and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])  # NotNext\n",
    "            negative += 1\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_padding_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "\n",
    "    # # eq(zero) is PAD token\n",
    "    padding_attention_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return padding_attention_mask.expand(batch_size, len_q, len_k)   # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.segment_embed = nn.Embedding(num_segments, hidden_size)\n",
    "        self.position_embed = nn.Embedding(max_length, hidden_size)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, segment):\n",
    "        seq_len = x.size(1)\n",
    "        position = torch.arange(seq_len, dtype=torch.long)\n",
    "        position = position.unsqueeze(0).expand_as(x)\n",
    "        embedding = self.token_embed(x) + self.segment_embed(position) + self.position_embed(segment)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-product Attention\n",
    "\n",
    "BERT’s model architecture is a *multi-layer bidirectional Transformer encoder* based on the original implementation.\n",
    "\n",
    "The attentions is calculate by Scaled Dot-product Attention:\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = \\text{softmax}(\\frac{Q\\cdot K^T}{\\sqrt{d_k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def farward(self, Q, K, V, attention_mask):\n",
    "        # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        # 对K的最后两个维度转置，即 序列长度 len_k 和 向量维度 d_k\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "\n",
    "        # Fills elements of self tensor with value where mask is one.\n",
    "        # 对 attention_mask 中标记为 1 的位置，填充极小的值（这里是 -1e9，非常接近负无穷）\n",
    "        # 为了“掩盖”某些无效的序列元素，如填充的 [PAD] token\n",
    "        # 在计算注意力分数时，不希望注意力头对填充的部分（如序列中无意义的 PAD token）产生任何关注，所以用非常小的数来屏蔽这些位置，防止它们对结果产生影响。\n",
    "        # 由于我们用的激活函数是 Softmax，小数值会让这些位置的权重接近于 0，因为 Softmax 使得更大的数对应较大的权重，小的数则几乎被忽略。使得这些位置不会对注意力机制产生影响。\n",
    "        scores.masked_fill_(attention_mask, -1e9)\n",
    "        \n",
    "        # dim=-1: 在 scores 最后一个维度 softmax\n",
    "        # 对每个查询 Q 对应的所有键 K 计算注意力权重。最后一维通常是 序列长度方向，即每个查询点的注意力分布。\n",
    "        attention = nn.Softmax(dim=-1)(scores)  # 注意力权重，即 查询向量 Q 对不同 键向量 K 的关注程度\n",
    "        context = torch.matmul(attention, V)  # 最终结果，即 经过注意力机制加权之后的值向量 V\n",
    "        \n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention\n",
    "\n",
    "查询向量$Q$、键向量$K$、和值向量$V$最初是从同一个输入向量获得的。经过多头的并行处理之后，它们的输出会被合并，以便传递给后续的网络层。\n",
    "\n",
    "1. 线性变换生成多头 $Q$, $K$, $V$\n",
    "2. 拆分为多头并行计算\n",
    "3. 各头独立计算注意力\n",
    "4. 合并多头输出\n",
    "    - `.transpose(1, 2)` 将维度恢复到 `[batch_size, seq_len, n_heads, d_v]`\n",
    "    - `.contiguous()` 保证内存连续， 用`.view()` 将多头的输出结果拼接成原始输入形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        nn.Linear() 定义一个全连接层（线性变换）output = input · W^T + b\n",
    "        将 输入的查询向量 Q 从 hidden_size 映射到 d_k * n_heads\n",
    "        '''\n",
    "        self.W_Q = nn.Linear(hidden_size, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(hidden_size, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(hidden_size, d_v * n_heads)\n",
    "\n",
    "    def forward(self, Q, K, V, attention_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        # 将 Q K V 映射成 多头自注意力 的形状\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)  # k_s: [batch_size x n_heads x len_q x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)  # v_s: [batch_size x n_heads x len_q x d_v]\n",
    "\n",
    "        '''\n",
    "        扩展并复制 attention_mask 以适应多头注意力机制的输入格式\n",
    "        多头注意力机制中的每个头都需要一个对应的掩码，因此要为每个头复制相同的 attention_mask\n",
    "        以确保在所有头上应用 相同的掩码 来 屏蔽不必要的序列元素（如 [PAD] token)\n",
    "        '''\n",
    "        # unsqueeze(1): [batch_size, len_q, len_k] -> [batch_size, 1, len_q, len_k] 在指定的维度 dim 上插入一个新的维度，改变张量的形状，但不改变数据\n",
    "        # repeat(): [batch_size, 1, len_q, len_k] -> [batch_size, n_heads, len_q, len_k]\n",
    "        #           操作会沿指定的维度重复张量的内容。每个参数表示要在相应维度上重复多少次。\n",
    "        #           将 attention_mask 在第二个维度上重复 n_heads 次，也就是复制出 n_heads 个相同的掩码，用于多头注意力的每一个头。\n",
    "        attention_mask = attention_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)  # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attention = ScaledDotProductAttention()(q_s, k_s, v_s, attention_mask)\n",
    "        \n",
    "        '''\n",
    "        将多头注意力的输出结果 重新组合为 原始的形状\n",
    "        '''\n",
    "        # contiguous(): 确保张量在内存中是连续的, transpose() 操作不会改变张量在内存中的存储方式，只是改变视图，有时会导致不连续的内存布局\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)   # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, hidden_size)(context)\n",
    "        return nn.LayerNorm(hidden_size)(output + residual), attention  # output: [batch_size x len_q x d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed Forward Network\n",
    "\n",
    "Apply a feed-forward network to each position of the sequence independently.\n",
    "\n",
    "#### GELU\n",
    "\n",
    "ELU (Gaussian Error Linear Unit) activation function, which is a smooth, non-linear activation function commonly used in modern Transformer architectures like BERT.\n",
    "\n",
    "Unlike the more common ReLU or Leaky ReLU, GELU offers __a probabilistic and smoother transition__, which helps in handling small input values better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"Implementation of the gelu activation function by Hugging Face\"\n",
    "    return x * 0.5 * (1. + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    BERT Encoder\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder_self_attention = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, encoder_input, encoder_self_attention_mask):\n",
    "        encoder_output, attention = self.encoder_self_attention(\n",
    "            # Q K V attention_mask\n",
    "            encoder_input, encoder_input, encoder_input, encoder_self_attention_mask\n",
    "        )\n",
    "        encoder_output = self.pos_ffn(encoder_output)\n",
    "        return encoder_output, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Encoder() for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.active1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.active2 = gelu\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, 2)\n",
    "\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.token_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, maked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        encoder_self_attention_mask = get_attention_padding_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, encoder_self_attention = layer(output, encoder_self_attention_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_model, d_model]\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.active1(self.fc(output[:, 0]))  # [batch_size, hidden_size]\n",
    "        logits_clsf = self.classifier(h_pooled)  # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))  # [batch_size, max_pred, d_model]\n",
    "        # get masked postiiton from final output of the Transformer\n",
    "        h_masked = torch.gather(output, 1, masked_pos)\n",
    "        h_masked = self.norm(self.active2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "\n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m make_batch()\n\u001b[0;32m----> 6\u001b[0m input_ids, segment_ids, masked_tokens, masked_pos, isNext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(torch\u001b[38;5;241m.\u001b[39mLongTensor, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(text)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
