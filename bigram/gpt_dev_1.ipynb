{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2GANY5omzxbX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-08-17 01:20:54--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-08-17 01:20:54 (20.1 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# connect to G_drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# download tiny shakespeare texts\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXwPuAM197SI",
        "outputId": "61b92226-b24e-4d35-f80e-1a8d728a2971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "# load text dataset\n",
        "with open('ninput.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knc_lPgx-Fu9",
        "outputId": "822fdbce-efce-4f00-df0a-71bee718e820"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(65, \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get all unique characters appears in the corpus, sorted\n",
        "chars = sorted(list(set(c for c in text)))\n",
        "len(chars), ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2lajPEX-e0_",
        "outputId": "50860d1e-78b7-4c3a-b085-e3189ef0d2e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# write a encoder and decoder\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxXBWFdg_kTn",
        "outputId": "3c4031ab-7f10-4297-d3ab-ca374b39075e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# split\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)  # encode text and save to dataset\n",
        "N = int(0.9*len(data))  # 90% training and the rest for validation\n",
        "train_data = data[:N]\n",
        "val_data = data[N:]\n",
        "\n",
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4yPDuJWuonw",
        "outputId": "9f8479da-9df3-4261-e816-b02ee62a9a39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 1, 58, 46, 53, 59,  6,  1, 53],\n",
              "         [42,  1, 57, 53,  1, 51, 59, 41],\n",
              "         [ 1, 58, 46, 43,  0, 39, 44, 58],\n",
              "         [10,  0, 32, 46, 43,  1, 52, 43]]),\n",
              " tensor([[58, 46, 53, 59,  6,  1, 53, 56],\n",
              "         [ 1, 57, 53,  1, 51, 59, 41, 46],\n",
              "         [58, 46, 43,  0, 39, 44, 58, 43],\n",
              "         [ 0, 32, 46, 43,  1, 52, 43, 61]]))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# data loading\n",
        "torch.manual_seed(5525)\n",
        "batch_size = 4\n",
        "\n",
        "# generate a small batch of data of inputs x and targets y\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  # generates a tensor ix containing batch_size random integers as starting indices\n",
        "  # to extract blocks of text from data tensor\n",
        "  # subtract block_size to ensure enough data remaining to extract a block of size\n",
        "  # i.e., i + block_size should not exceed the length of the data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "\n",
        "  # stack 1d datas together\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "\n",
        "  # x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "# xb.shape, yb.shape  # (torch.Size([4, 8]), torch.Size([4, 8]))\n",
        "xb, yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj0joOfr1plO",
        "outputId": "cb88edc1-b2f5-4052-910c-f1efc13873f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 65]) tensor(4.6213, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "JvtfTNnOP\n",
            "d snF!CC&DZ-OjRICJPUiIdoZBLYeR'Vc?ob\n",
            "ivOqVd kNXQ;Dp\n",
            "d &DlMkLws?QGcofLY:q;Bhp?Eyggn;OIpNQnS\n"
          ]
        }
      ],
      "source": [
        "# bigram\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(5525)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # the lookup table\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx and targets are both (B, T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx)  # (B, T, C), the scores for next character in the sequence\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)  # (B * T, C)\n",
        "      targets = targets.view(B*T)  # (B * T) or targets = targets.view(-1)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx)\n",
        "      # get last time step\n",
        "      logits = logits[:, -1, :]  # (B, C)\n",
        "      # softmax to get probs\n",
        "      probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "      # predict next index by sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "      # append new idx to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "vocab_size = len(chars)\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "# out = m(xb, yb) # out.shape  # torch.Size([4, 8, 96])\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape, loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s85AXD0e2vB8",
        "outputId": "bc6d4ee2-417a-4500-b88f-a26147615557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.474010467529297\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "\n",
        "for steps in range(10000):\n",
        "  # get a training data batch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wBQmIvm9WcN",
        "outputId": "0738f195-a3e0-4e0b-bcb4-c776f75fb919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "IXDinopeioreve agerpllin-qupNGHI's be Pke wit fase scerof dstho wnde t O:\n",
            "KINT:WAn sthag m coul kis, myor to ayHu be athise nge ncofooly s man ang ves! EN nourtietts, at '!\n",
            "G w:\n",
            "\n",
            "Laere tue uto a; spo bereven o and s padrvern,\n",
            "Th blds hin'se,\n",
            "Myow cabeso st gy te as:\n",
            "\n",
            "\n",
            "I'n wer touree sowhe, oband--w ind,\n",
            "Whityond bala bor, m?\n",
            "c.\n",
            "hed t t ang CENVerous.\n",
            "Habl yepilen msevee\n",
            "T:\n",
            "MENEveare en, b, IO: tothokmifasOLAgre w, frthat, m prr's heangh orcheswet y m thandeanot atreeroulllero dare. andrditha af lf t sthis s onowerdy chas go,\n",
            "is,\n",
            "Thal nclen, f thy be s, s, geres igpt's t t men ne m; igoeverpf hienoursus, uis ates folllle meel pthak m pthe oris.\n",
            "VO:\n",
            "Mang ascos ay I den'bs\n",
            "I pr spr athaco amy\n",
            "I NRAMAUESorerar.\n",
            "Wancre s s\n",
            "hangowoweanthe, whow ono thitheevanthil w thouseton;\n",
            "ad\n",
            "Yofof outhotl'ds;\n",
            "\n",
            "O: homy omee erchin bencide, SLE her ckier blfomenomy not oulerld.\n",
            "Fods, hyseadergoudlekncen\n",
            "\n",
            "\n",
            "Wha! y ine I see; y trve an y, kerdon ply eno;\n",
            "ARonkes, s wirt s.\n",
            "\n",
            "Tatetrd t ppratort, le omou Swile tin thide d out, thise my g s?\n",
            "Pot,\n",
            "Word d wow'lis poty tathe ckex am oofe:\n",
            "By it owande.\n",
            "BEd ho-r indar h pif mepanoup had tide'd s,\n",
            "IAny? wo ous keas sto ak w fou s thear conjoupaerurPout touly.\n",
            "wenen nthisthnd be hes ay d lin tst tis M$$! at.\n",
            "Th mis.\n",
            "D:\n",
            "\n",
            "ORWhr mamisooriee wazlll hind\n",
            "AMENEalene hiswearen athe hoor, he hear hKEThersten he ll ha INE nd m k, pul  mu\n",
            "' ore f, gesarofon, suts\n",
            "Whundok, r hirily r bar,\n",
            "AQUS:\n",
            "An s's ent by.\n",
            "Dus ts,\n",
            "HUCor?\n",
            "\n",
            "\n",
            "As,\n",
            "s, tec-\n",
            "B!HEX; tove el lle, birere mayishandyooIFicathe dolil; S: cantomoivee g?\n",
            "PEat y, fotscerd e 'd me t lons &,'g\n",
            "OLALO lve s ce.\n",
            "Fean\n",
            "Thearmond\n",
            "Nuseklon my m the thepeewecathouscadave'thengyowory ENVER:\n",
            "ALYr hiof difryor me Es nioun,\n",
            "\n",
            "Th t muse al sh, be ndsamulecee MIOULAROn thind el he wd y ffour bo prse.\n",
            "TITI ghind,\n",
            "Sarlidestllt.\n",
            "Hore ors hoy\n",
            "OLUPeaghailloumagou qune, weig, hldifor goace\n",
            "Twat ldisid me the Y:\n",
            "\n",
            "Tirs pemyodse tay s!\n",
            "IORDu, lallas mard\n",
            "\n",
            "A:\n",
            "Heak he&w umbrels\n",
            "TENGele:\n",
            "s ange imy beemald, wacke wn:\n",
            "ENawiget te,\n",
            "BLYoterounsr athiowiss halmio d t th tooy coolichisipe htooty bendrvwer'dgsurere d borof t hy tit a clowom marealys, hag bl h mugel t fis len, asl bdwe hindiowhare th nevere, n d d o wir, ono y m immancchyovil, th, hoourd fit dwibeankind Hy n s E:\n",
            "Thar in? ssh,\n",
            "Mthay:\n",
            "Treare ly. ince d whosut,\n",
            "Whtono no s'HBule red hy I he\n",
            "BOr derposiswallliloviee memigere, iofand of o FRMyseadermu oromame.\n",
            "AMan merol me tage y d twhenst CKENoppe her! te sts t y fa! hed t in\n",
            "\n",
            "ARELY:\n",
            "Peeupste celvemy garwe tharst e g by slowe an fem orowothe,\n",
            "ME G f ddo t,\n",
            "SThousto--at---INVus im,\n",
            "\n",
            "LYet?\n",
            "r sonstld pareg, l? teran,-su st\n",
            "cutico g, n d wongdit dere\n",
            "KEN hapre:buno wabem;\n",
            "Tbeshrind asKICHNNCaimetealtut sple lll, ts:\n",
            "Pow tod.\n",
            "TAits sthan wer,\n",
            "Thatar ar.\n",
            "QUKINV:\n",
            "Do weore hes:?\n",
            "Pl. cisp ENEWind blaloutrle.vepr yons d me hathifat wnt'se, INI Pr\n",
            "FI he ay s bu uo ly h mo itiet a s ss ICADad merks de bonot'I'medenryad sheco'd berotaboom. f thir: ile an f th tsthend iree RK--Cine INouce sely the heay avy medeveral f?\n",
            "TIFOMEir,\n",
            "weve.\n",
            "\n",
            "Py ou th r: it poueave yi'IVET:\n",
            "Noour!\n",
            "Y ch no heareventy rou ak beld; dr ithawevinthan achatiscrethe thered chesty, in Bu go sthe be metestrive p t s?\n",
            "PUDomau l bl Fit y bbe?\n",
            "Soulowr coranenom drt m slingstedin miman EGLlta wiet by S:\n",
            "Muend wnter te list.\n",
            "MPUCOFr trd g chere ws than dillet, t.\n",
            "Fied d hut mat an ary,ha ss hamup, thba ts y alout\n",
            "ROVOLIOFOFllit, haroorot veg y, stere tteof t r, so thaindive ndssary.\n",
            "Wicofaity, rst we Mavy'dit uth d tet ple, y t manoagh, s, aton, y ws us ff.\n",
            "\n",
            "ive hendous ayo y ethn asit ff ir ugre wousl'othif\n",
            "II dethefesowidiouiolours saval INGRore tes t outar y ltrerverd bothay acel hith INUSur,\n",
            "BO:\n",
            "T: he\n",
            "INESest thonse mp VI d cese cad mbay e thou?\n",
            "INTyale gealingr seens tr atoand IN:\n",
            "NX&ze\n",
            "\n",
            "DIClk t fals anit toof moree cuerunses'sates so e t,\n",
            "Nouseve chy p muls\n",
            "G de, &:\n",
            "CHE:\n",
            "Tob'Stl,\n",
            "Whaneshe squgsthe dofll anoreshars s ave!\n",
            "Bupevienge das:\n",
            "ISClllso m l re too sh\n",
            "ADortha t p aingre gs wind, be ks hakin p mecaCErand w'd;\n",
            "T:\n",
            "ARonoatithod patir tho anty, mpe, y.\n",
            "Wisise bataks! rant JYCl m wer t\n",
            "\n",
            "TOKicthie orillk se youkie me ld g th wld is,\n",
            "Wh itor hen, t LYor egod d athem me fXFFO, gert benghene wit, t scer;\n",
            "BLI o?\n",
            "YCKESSims\n",
            "Br d:\n",
            "N perdson;VI omonokee:\n",
            "\n",
            "\n",
            "od bath ichirined seat s petheswe II'SAR:\n",
            "tus-land te than aquron:\n",
            "\n",
            "O:\n",
            "bulg ke meas,--ceay th RY:\n",
            "NCING ces le, ie\n",
            "NThie bod l gofig RWhistantigaende, tout str,\n",
            "TENouers n co, sed ad nt, try s,\n",
            "Kf f trilapurdind phewa tthourerhim ne ptongenknd, pr r-hartondl O mur'sxf dllll.\n",
            "\n",
            "Anzu st by y w Gelllofomishe s, t qPUKin th t shaitofrd at asans, hen me t e how h Tist:\n",
            "A:\n",
            "HAntewimutr, eve orised? ma ts t, cease t ches t:\n",
            "IOwad! f ctrebear SI yoshalimarm cte,\n",
            "TClterea favegheshe spou Ruthin INore wis.\n",
            "AUK: larend wacooop, I ghesovOus h linte ang.\n",
            "Profourct ureetel t, mandixPur Jful m tochedous wo RIColo?---qu S: and meajoy f ff heto, rore lk lthoubellonout, inslalpld cou, tiliunty s at; drest orewin brsser do ble\n",
            "PSoumalichathist he, h fanens\n",
            "\n",
            "T:\n",
            "I uthto,\n",
            "\n",
            "Le-qurd\n",
            "\n",
            "E:\n",
            "Theef\n",
            "Mor IUNUCit, I whey ar\n",
            "AUCEmounk an: hich's, kiur\n",
            "\n",
            "Whe le lot\n",
            "LUSIUMathe?\n",
            "Treme ndpathind y womoler ndswee as\n",
            "INIllisy wn-wr the tis,\n",
            "An mitsery.\n",
            "\n",
            "MINTis le s:\n",
            "NGooupo'serst, hitll GMI ishat, thead woQWhe\n",
            "YOfllat forerveshon anderkelanuckerl treecelen bu I tey plld, the so he!\n",
            "kind 'skn orstouth\n",
            "LO s olameat te uth tesse--thr;\n",
            "SCA heno s,\n",
            "INORofou ie ee wind $sthepous ar yaretho beer heine'le cengoo tso onth,\n",
            "MNoftn\n",
            "DYou, asthe teve ashenoide, hendse\n",
            "AR: y te istong imakye t\n",
            "\n",
            "IO,\n",
            "INGNDIXELu aleerof.\n",
            "\n",
            "My hwofon t aldpe he\n",
            "\n",
            "toththot they veve,\n",
            "I tam t uthes LOHongh tot ne notors ares towa, he cel y bu t\n",
            "Lon t.\n",
            "TENCIDut whindin s nve y,\n",
            "Th s t hisstAno-lpsugdist tho bed n.\n",
            "rst tour fithe touref sas llsow teperd\n",
            "ICAlot-se wnh;\n",
            "I ther\n",
            "har,\n",
            "\n",
            "My, y by snve tiseasig hy t h d hean en ly halowalon th jer; wime ornitavee macta! de worcosur l f, oQUK:\n",
            "Frth d rs lours.'esawsthe. k t p s, menecepte.\n",
            "r? whe,? athes barare tcagr\n",
            "KI frccied, bowin as m 'd s s pstongoracow tout festord myoour cazelve hithey boonct pathangry mongan'd aserator fey kusatowir sh;'-USllls.\n",
            "Anderivis wn ciedeapbloowe holas ato it,\n",
            "\n",
            "merd! t lovethPins\n",
            "S:\n",
            "An t and paiveer: auprhenorewe, chalean aizefam,\n",
            "AMobe yor'mur ous Ongowehithithofred.\n",
            "\n",
            "\n",
            "Naknghechas antoregimillimanth\n",
            "Neliclth a nt therd\n",
            "n t ad sthiveccorulls IUFy t.\n",
            "This VE:\n",
            "T:\n",
            "nel?\n",
            "I fily,\n",
            "Wi'send, yfe-ntle wenspr athe d t tou ffereay'sue and arer gnt t IS:\n",
            "\n",
            "\n",
            "H f RETO,\n",
            "PORETLENusther; ssury loo nghofru higre yooves O kngothK:\n",
            "Serou ake\n",
            "ELI vat kstor d n he Youstond terome khait mu g he aneelive baverrge, my ng:\n",
            "DUKI tan nd JBiuthim:\n",
            "O, o t stro murnthoutou s thitetintillly.\n",
            "Thaserthofoound allo s\n",
            "Wha hou baste nntu?\n",
            "Y hur f th-sbe aiealt t! weil cemol tifflughomy h, t 's.\n",
            "AUatins hed.\n",
            "EQy y'sthsht do RINo sdit ay wd nou?\n",
            "NVIUMy'd t MEiter woupiunttesstheacat ece Heis mpp,\n",
            "Farinde h busely thive hisu ame machind er my cug me t atou art My Macchere ou,\n",
            "\n",
            "ARint.\n",
            "OFLAncor, s;\n",
            "Peilse desicee a\n",
            "\n",
            "ckince H:\n",
            "\n",
            "Y;\n",
            "hett mere'tleane the thainche.\n",
            "ICKINThe, t bowr-ntharet chourg. wer? giscoul pinve t fow;\n",
            "Desanofr, pothor mall't diSERFvit thary it knattothe hayan ty Lorans: bre mo goroond:\n",
            "Windellkimy t wheeanor aven m scowinsthis d have.\n",
            "Doful th s, de ghardeanollnd\n",
            "fon, ystr den ice listilie fo isout han byon's I'doug?\n",
            "?\n",
            "Bunonour asut te th, allinge& tougin her ot, wosbldesere thinorernontherendech, ucifort to mes ato te.\n",
            "Ifan co stharw s h, dopowe makeettut maved t p sors kisthil he, d tes t, d stol oblosovucond hoo,\n",
            "TEds:\n",
            "Afon:\n",
            "Win.\n",
            "Pre sonchat whereucond t hewire, thoig,$ERIO:\n",
            "FRI at, RGofueruers anfor m beantI'\n",
            "AMy qulldafiche h, thomowat houeenp o nd al ththeis ace gsacus nge, chile\n",
            "\n",
            "apalee thang' angg,\n",
            "A: ie htrlll thoce ILik!-d fos ke!--Whe hend, d\n",
            "Procoulliet p t icas, ll whand'?\n",
            "HUR in-s:\n",
            "Tiode hyokin,\n",
            "\n",
            "\n",
            "To meds t p to thetonelellpyor it.\n",
            "Nowfabo n ato mice, ost. IUCare.\n",
            "\n",
            "I's:\n",
            "\n",
            "ERD: cre Ye!nenonomithepthons GHNarfe, n ve n'lfod,L:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I ghathy ss tispazpppou nol t yomay, had;\n",
            "s'l myof ge o has\n",
            "Cwincke ikere!\n",
            "Jourayst,\n",
            "JULI IO:\n",
            "BEENGan!\n",
            "S:\n",
            "Yorse mmoofandearer ings fur t cise tilolamy:\n",
            "FIt blishol kENED:\n",
            "M:\n",
            "Ayom; CHind s\n",
            "\n",
            "RORRBUCANI't actu,\n",
            "Whour alor let bato pe; RKI worithateasine asst ty thor\n",
            "Pinco be r witour horely wolxfist:\n",
            "The athoomoulaprr he t t t nn thepee ioucester;\n",
            "\n",
            "\n",
            "VICIr'dathad brothar AUCOf lll,\n",
            "Ofus ola maveais?!CHo wis y ane ve yo n; Parthingeall'dsthpenomsar vOffor beaklith, INIper ubr bo ow thilsty ulerer. qu; lotartaver\n",
            "DUS: findenanoultow po!\n",
            "Czjncharerwesthiled f!\n",
            "ORou ir, simore shean: theat w!\n",
            "Wheidl!\n",
            "HAnd brenous tharffoutathitere Goon oprwin:\n",
            "Wherhourseresppe toumo lorillle gs br\n",
            "Toure fon rereers, t I: is Yole hat c!\n",
            "Yog acemuk, gu whiathabavalld.\n",
            "\n",
            "nom.\n",
            "I de d hithigey asd g'st ssheefr yor alfarin.\n",
            "biTESevemingallas,\n",
            "ERWhex tersoue, aicasETHesteyour wast lare s d IE:\n",
            "Ase lateners\n",
            "Grd IFLULKES w he'dsed and avemy'd anctesese avey grve?\n",
            "ARI Whise\n",
            "Withy we t cesil felewengutyo s hale y r?, t pepprir oone Whavecoutwisthe laboubur my,\n",
            "Wxpopowou bonorsut eare; inulothaNGedr, fll\n",
            "W:ELUSTE:\n",
            "T:g whet thiaces aknet's, t; ficily tet d imatimeamimean' womeatitherhit the geth-uno ifes d y s afe amave hat d!\n",
            "\n",
            "Tofrthe ar'den:\n",
            "?\n",
            "bld pr mar aratsuedetile ve ubot.\n",
            "As \n",
            "\n",
            "WIEDUCOULENS:\n",
            "Lake w f de yo ht the his t t s meyow'ld, s he\n",
            "TZd foverst. y\n",
            "\n",
            "'llout theretheim has t wen.\n",
            "OMyonserofanorir corave resho pisThave n theng lfe.\n",
            "Hitis w hepRoulf bshore:\n",
            "That icave t g magepulal lellthicohe isheana y ayover w'd lee hiert\n",
            "VORo l'demyo infe-ffor.\n",
            "Wh ave es meer y.\n",
            "Tove h\n",
            "KINR ho yewire rd iort,\n",
            "H:\n",
            "Mes handesstofo I ben\n",
            "My ithandipo s orser ath?\n",
            "I be, eo wivay thakewreyo feno I 's hy an f a f oveg w thben has ooutthotyower of.\n",
            "LARoese d I quaHinty houser, a asth mfave tan owe thJ'se,\n",
            "Hon:?\n",
            "\n",
            "Whan.\n",
            "\n",
            "\n",
            "Of d mofois!'torgboy the\n",
            "\n",
            "ALEd S:\n",
            "OMI ELE: t whesit e; hiss sthathe here as we fr thithiY go cous n han hf d,\n",
            "Wh we m whis.\n",
            "Aced; e dl:\n",
            "Tht bofan e te fant arinst, Co n\n",
            "CINUELYCOIOn g ld t kernd!\n",
            "Ave EENGLus torud! mpit wililizicieracarathank' ar, hinf, twomore\n",
            "\n",
            "\n",
            "LLomesewe t olo chas\n",
            "Whosches isherd tul\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Math trick of self-attension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(5525)\n",
        "B, T, C = 4, 8, 2  # batch, time, channels\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UTVgl-dV9unI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 8]), torch.Size([4, 8, 2]))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 1\n",
        "weight = torch.tril(torch.ones(T, T))  # create a TxT lower triangular matrix\n",
        "weight = weight / weight.sum(1, keepdim=True)  # every row is averaged by col numbers \n",
        "res = weight @ x  # average by column per row (T, T) @ (B, T, C) --boardcast--> (B, T, C)\n",
        "weight.shape, res.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version2: softmax\n",
        "from torch.nn import functional as F\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))  # create a TxT lower triangular matrix\n",
        "weight = torch.zeros((T, T))\n",
        "weight = weight.masked_fill(tril == 0, float('-inf'))  # masked fill the weight matrix at place where in tril==0 with -inf\n",
        "# weight = F.softmax(weight, dim=-1)  # use softmax to get averaged by row\n",
        "# res2 = weight @ x\n",
        "weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weight = F.softmax(weight, dim=-1)  # use softmax to get averaged by row\n",
        "weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res2 = weight @ x\n",
        "torch.allclose(res, res2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self-attension\n",
        "\n",
        "**Scaled Dot-Product Attension**\n",
        "\n",
        "$$\n",
        "\\text{Attension}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "divided by $\\sqrt{d_k}$ to avoid too sharpy after softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "head_size = 16\n",
        "\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)  # (B, T, 16)\n",
        "q = query(x)  # (B, T, 16)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00],\n",
              "        [9.0661e-01, 9.3387e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00],\n",
              "        [1.1255e-01, 8.4306e-01, 4.4397e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00],\n",
              "        [2.5565e-01, 1.3902e-01, 3.8473e-01, 2.2060e-01, 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00],\n",
              "        [3.6798e-02, 1.1804e-04, 9.5667e-01, 6.4098e-03, 6.6768e-06, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00],\n",
              "        [5.8463e-02, 2.4333e-01, 2.7453e-02, 9.3168e-02, 5.2570e-01, 5.1882e-02,\n",
              "         0.0000e+00, 0.0000e+00],\n",
              "        [1.9701e-01, 5.4503e-02, 3.3448e-01, 1.1815e-01, 2.3135e-02, 2.1201e-01,\n",
              "         6.0718e-02, 0.0000e+00],\n",
              "        [5.5008e-02, 1.8497e-01, 2.9514e-02, 8.2761e-02, 3.6395e-01, 4.9922e-02,\n",
              "         1.8261e-01, 5.1268e-02]], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Attension is a communication mechanism.\n",
        "Init is a set of vectors.\n",
        "Each batch dimension is independent.\n",
        "self-attension: keys, queries, values all come from same source\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
